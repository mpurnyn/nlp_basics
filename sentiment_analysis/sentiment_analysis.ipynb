{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dataset http://help.sentiment140.com/home\n",
    "- 1.6 million tweets alongside their respective sentiment (0 for negative and 4 for positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "MAXLEN = 16\n",
    "TRUNCATING = 'post'\n",
    "PADDING = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "MAX_EXAMPLES = 160000\n",
    "TRAINING_SPLIT = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data point looks like this:\n",
      "\n",
      "\"0\",\"1467810369\",\"Mon Apr 06 22:19:45 PDT 2009\",\"NO_QUERY\",\"_TheSpecialOne_\",\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"\n",
      "\n",
      "Second data point looks like this:\n",
      "\n",
      "\"0\",\"1467810672\",\"Mon Apr 06 22:19:49 PDT 2009\",\"NO_QUERY\",\"scotthamilton\",\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SENTIMENT_CSV = \"./training_cleaned.csv\"\n",
    "\n",
    "with open(SENTIMENT_CSV, 'r') as csvfile:\n",
    "    print(f\"First data point looks like this:\\n\\n{csvfile.readline()}\")\n",
    "    print(f\"Second data point looks like this:\\n\\n{csvfile.readline()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing the raw data\n",
    "\n",
    "Now you need to read the data from the csv file. To do so, complete the parse_data_from_file function.\n",
    "\n",
    "A couple of things to note:\n",
    "\n",
    "- dont omit first line\n",
    "- use list not numpy array\n",
    "- To read from csv files use csv.reader by passing the appropriate arguments. csv.reader returns an iterable that  returns each row in every iteration. So the label can be accessed via row[0] and the text via row[5].\n",
    "- convert labels to int 0 (neg) and 1 (pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_file(filename):\n",
    "    \"\"\"\n",
    "    Extracts sentences and labels from a CSV file\n",
    "    \n",
    "    Args:\n",
    "        filename (string): path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename, 'r') as csvfile:\n",
    "        ### START CODE HERE\n",
    "        reader = csv.reader(csvfile, delimiter=\",\")\n",
    "        \n",
    "        for row in reader:\n",
    "            label = row[0]\n",
    "            if label == \"0\":\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "            text = row[5]\n",
    "            sentences.append(text)\n",
    "        \n",
    "        ### END CODE HERE\n",
    "        \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8f in position 2210: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ppurn\\workspace\\python_experiments\\nlp_basics\\sentiment_analysis\\sentiment_analysis.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m# Test your function\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000010?line=1'>2</a>\u001b[0m sentences, labels \u001b[39m=\u001b[39m parse_data_from_file(SENTIMENT_CSV)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000010?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdataset contains \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(sentences)\u001b[39m}\u001b[39;00m\u001b[39m examples\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000010?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mText of second example should look like this:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msentences[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\ppurn\\workspace\\python_experiments\\nlp_basics\\sentiment_analysis\\sentiment_analysis.ipynb Cell 6'\u001b[0m in \u001b[0;36mparse_data_from_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000006?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m csvfile:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000006?line=15'>16</a>\u001b[0m     \u001b[39m### START CODE HERE\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000006?line=16'>17</a>\u001b[0m     reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(csvfile, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000006?line=18'>19</a>\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m reader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000006?line=19'>20</a>\u001b[0m         label \u001b[39m=\u001b[39m row[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000006?line=20'>21</a>\u001b[0m         \u001b[39mif\u001b[39;00m label \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ppurn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8f in position 2210: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "sentences, labels = parse_data_from_file(SENTIMENT_CSV)\n",
    "\n",
    "print(f\"dataset contains {len(sentences)} examples\\n\")\n",
    "\n",
    "print(f\"Text of second example should look like this:\\n{sentences[1]}\\n\")\n",
    "print(f\"Text of fourth example should look like this:\\n{sentences[3]}\")\n",
    "\n",
    "print(f\"\\nLabels of last 5 examples should look like this:\\n{labels[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that this dataset contains a lot of examples. In order to keep a low execution time of this assignment you will be using only 10% of the original data. The next cell does this while also randomnizing the datapoints that will be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ppurn\\workspace\\python_experiments\\nlp_basics\\sentiment_analysis\\sentiment_analysis.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m# Bundle the two lists into a single one\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000007?line=1'>2</a>\u001b[0m sentences_and_labels \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(sentences, labels))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000007?line=3'>4</a>\u001b[0m \u001b[39m# Perform random sampling\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ppurn/workspace/python_experiments/nlp_basics/sentiment_analysis/sentiment_analysis.ipynb#ch0000007?line=4'>5</a>\u001b[0m random\u001b[39m.\u001b[39mseed(\u001b[39m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# Bundle the two lists into a single one\n",
    "sentences_and_labels = list(zip(sentences, labels))\n",
    "\n",
    "# Perform random sampling\n",
    "random.seed(42)\n",
    "sentences_and_labels = random.sample(sentences_and_labels, MAX_EXAMPLES)\n",
    "\n",
    "# Unpack back into separate lists\n",
    "sentences, labels = zip(*sentences_and_labels)\n",
    "\n",
    "print(f\"There are {len(sentences)} sentences and {len(labels)} labels after random sampling\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33cd822c57ff6bb37babb058289896aa3fa85c236acdda9cd4396890871775c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
