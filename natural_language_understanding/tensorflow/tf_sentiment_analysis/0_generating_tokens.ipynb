{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "In most NLP tasks, the initial step in preparing your data is to extract a vocabulary of words from your corpus (i.e. input texts). You will need to define how to represent the texts into numerical representations which can be used to train a neural network. These representations are called tokens and Tensorflow and Keras makes it easy to generate these using its APIs. You will see how to do that in the next cells.\n",
    "``` \n",
    "-- Coursera NLP Tensorflow Week 1\n",
    "\n",
    "#### Word based encodings\n",
    "\n",
    "Letters can be encoded with ASCII, but trying to make sense of the encoding can be quiet difficult because letters dont have as much meaning as words in a sentence\n",
    "\n",
    "So lets try creating word based encodings and passing that to a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#some random sentences \n",
    "sentences = [\"I love my dog\", \"I love my cat\"] \n",
    "\n",
    "tk = Tokenizer(num_words=100) #keep up to 100-1 words based on freqency\n",
    "tk.fit_on_texts(sentences) #input text to tokenize\n",
    "widx = tk.word_index #return the generated tokens. not affected by num_words\n",
    "print(widx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind\n",
    "- all punctuation is ignored \n",
    "- words are converted to lower case. \n",
    "\n",
    "You can override these behaviors by modifying the **filters** and **lower** arguments of the Tokenizer class.\n",
    "```\n",
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    analyzer=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "--- Tensorflow API website"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33cd822c57ff6bb37babb058289896aa3fa85c236acdda9cd4396890871775c9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
